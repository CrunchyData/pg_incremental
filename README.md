# pg\_incremental: Fast, Reliable, Incremental Insert Processing in PostgreSQL

When storing a stream of event data in PostgreSQL (e.g. IoT, time series), a common challenge is to process only the new inserts. For instance, you might want to create one or more rollup tables containing pre-aggregated data, and insert and update the aggregates as new data arrives. However, you cannot really know the data that is still being inserted by concurrent transactions, and immediately aggregating data when inserting (e.g. via triggers) is certain to create a concurrency bottleneck. When periodically repeating an aggregation, you also want to make sure that events are processed successfully exactly once, even when queries fail.

`pg_incremental` is a simple extension that helps you do fast, reliable, incremental batch processing of new rows in a PostgreSQL table.

## Creating incremental processing pipelines

To use `pg_incremental`, you need to have a bigserial / identity or timestamp column in your table, preferably with an index (e.g. btree / primary key, BRIN):

For example, consider this source table and rollup table:
```sql
-- create a source table
create table events (
  event_id bigint generated always as identity,
  event_time timestamptz default now(),
  client_id bigint,
  path text,
  response_time double precision
);
create index on events using brin (event_id);
create index on events using brin (event_time);

-- generate some random inserts
insert into events (client_id, path, response_time)
select s % 100, '/page-' || (s % 3), random() from generate_series(1,1000000) s;

-- create a rollup table to pre-aggregate the number of events per day
create table events_agg (
  hour timestamptz,
  event_count bigint,
  primary key (hour)
);

```

There are two ways of creating a pipeline to incrementally aggregate new rows in the `events` table into the `events_agg` table:

- Sequence pipelines - The pipeline query is executed with a range of sequence values
- Time interval pipelines - The pipeline query is executed for a range of time intervals, after those time intervals have passed.

Which pipeline is most suitable depends on your requirements and scenario. In general, sequence pipelines are more suitable for aggregating by a timestamp column that is not generated by the database itself (and you may have late data), since it will process rows in insertion order regardless of the timestamp. Time interval pipelines are more appropriate for processing the data in fixed size intervals, but are less tolerant to late data.

### Creating a sequence pipeline

You can define a sequence pipeline with the `incremental.create_sequence_pipeline` function by giving it the name of a source table name with a sequence, or an explicit sequence name. The query you pass will be executed in a context where $1 and $2 are set to the lowest and highest value of a range of sequence values that can be safely aggregated.

```sql
-- create a pipeline to aggregate new inserts from a postgres table using a sequence
-- $1 and $2 will be set to the lowest and highest (inclusive) sequence values that can be aggregated

select incremental.create_sequence_pipeline('event-aggregation', 'events', $$
  insert into events_agg
  select date_trunc('day', event_time), count(*)
  from events
  where event_id between $1 and $2
  group by 1
  on conflict (day) do update set event_count = events_agg.event_count + excluded.event_count;
$$);
```
Creating a pipeline automatically sets up a [pg_cron](https://github.com/citusdata/pg_cron) job that runs every minute by default (configurable) and runs the insert..select with an unprocessed sequence range. You can define multiple pipelines for the same source table to create different aggregations. 

The pipeline execution ensures that the range of sequence values is safe, meaning that there are no more transactions that might produce sequence values that are within the range (it waits for ongoing transactions before proceeding with the command). The size of the range is effectively the number of inserts since the last time the pipeline was executed up to the moment that the new pipeline execution started.

The benefit of sequence pipelines is that they can process the data in small incremental steps and it is agnostic to where the timestamps used in aggregations came from (i.e. late data is fine). The downside is that you almost always have to merge aggregates using an ON CONFLICT clause, and there are situations where that is not possible (e.g. exact distinct counts).

### Creating a time interval pipeline

You can define a time interval pipeline with the `incremental.create_time_interval_pipeline` function by giving it the name of a source table name with a time column. The query you pass will be executed in a context where $1 and $2 are set to the lowest and highest value of a range of sequence values that can be safely aggregated, because the pipeline execution makes sure that all ongoing transactions are only genearting higher values by waiting for table locks. 


Example:
```sql
-- create a pipeline to aggregate new inserts from a postgres table using a specified time interval
-- $1 and $2 will be set to the start and end (exclusive) of a range of time intervals that can be aggregated
select incremental.create_time_interval_pipeline('event-aggregation', '1 day', $$
  insert into events_agg
  select event_time::date, count(*)
  from events
  where event_time >= $1 and event_time < $2
  group by 1
$$);
```

The command will be executed once per time interval. If multiple intervals have passed since the last pipeline execution, each will be processed in a separate transactions, but as part of the same execution. It is also possible to execute the command for a range of intervals at once by supplying a `batched := true` argument. In that case, the command usually needs to group by the time interval.

The pipeline execution logic ensures that the range of time intervals is safe, _if the timestamp is generated by the database using now() and assuming no large clock jumps_ (usually safe in cloud environments). The size of the range is always a multiple of the time intervals. A time interval is processed after at least a minute has passed (configurable).

The benefit of time interval pipelines is that they can do more complex processing such as exact distinct counts and are also more suitable for periodically exporting data because the command always processed the same time range. The downside is that it you need to wait until after a time interval passes to see results and inserting old timestamps may cause data to be skipped. Sequence pipelines are more reliable in that sense.

## Resetting an incremental processing pipelines

If you need to rebuild an aggregation you can reset a pipeline to the beginning.
```sql
-- Reset a rollup
begin;
truncate events_agg;
select incremental.reset_pipeline('event-aggregation');
commit;
```
The next time the pipeline runs, the start of the sequence value range will be set to 0 to reprocess all rows.

## Dropping an incremental processing pipelines

When you are done with a pipeline, you can drop it using `incremental.drop_pipline(..)`:
```sql
-- Drop the pipeline
select incremental.drop_pipeline('event-aggregation');
```
